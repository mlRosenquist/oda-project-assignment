Comparing the performance of classification of the full features and to two features, it is clear that they distinguish. The accuracy is much lower using two features. As an example for Nearest Neighbor on the MNIST data set the accuracy for full features is 96.88\% while being 42.43\% on two features. Furthermore the training time is way faster on the lower features. Looking at the same example it takes 14.173 seconds to train the full features, but only 0.207 seconds on two features. It's seen on Figure \ref{fig:mnist-boundary-pbp} that there is only seven classes, meanwhile there is supposed to be ten. As shown in the confusion matrix on Figure \ref{fig:mnist-confusion-pbp-2} this clearly leads to test data being misclassified, where some classes are not represented at all. As PCA only looks into the variance of variables and not including response variables, it might have been better for the accuracy to utilize linear discriminant analysis instead. It's seen that the classifiers make different decision boundaries and vary in performance. For the MNIST data set Nearest Neighbor has the highest accuracy while being the slowest. Nearest 2 Sub-Class has the highest accuracy for both data sets with reduced features.           