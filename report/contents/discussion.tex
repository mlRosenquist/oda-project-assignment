Comparing the performance of classification of the full features and to two features, it is clear that they distinguish. The accuracy is much lower using two features. As an example for Nearest Neighbor on the MNIST data set the accuracy for full features is 96.88\% while being 42.43\% on two features. Furthermore the training time is in general way faster on the lower features. The time can't directly be compared between classifiers, since some utilize parallelism. Looking at perceptron with BP for MNIST the full features took ~6s while on two features only ~0.1s. It's seen on Figure \ref{fig:mnist-boundary-pbp} that there is only eight classes, meanwhile there is supposed to be ten. As shown in the confusion matrix on Figure \ref{fig:mnist-confusion-pbp-2} this clearly leads to test data being misclassified, where some classes are not represented at all. As PCA only looks into the variance of variables and not including response variables, it might have been better for the accuracy to utilize linear discriminant analysis instead. It's seen that the classifiers make different decision boundaries and vary in performance. Furthermore it's seen that classifiers that perform well on the MNIST data set, does not necessarily perform well on the ORL data set. 