This thesis will analyze the Image Classification problem. Utilizing only supervised learning algorithms This revolves around training a classification model with train images and thereafter being able predict correct classes for test images. The computer doesn't see the physical images as humans will, instead it sees a vector or matrix of pixels. The used classification models is described in the following sections.     

\subsection{Nearest Centroid}
Nearest Centroid, also called Nearest Prototype, calculates the centroids of each class and then predicts an observation to a class whose mean is the closest. Given a set of N samples, where each sample is represented by a vector $\mathbf{x}_i\in\mathbbm{R}^D$ and the corresponding labels are denoted as $l_i$. The centroids are denoted as $\mathbf{\mu}_k, k=1,...,K$ where $K$ is the amount of classes. Then during training the centroids are defined\cite{ai-course-notes}: 

\begin{equation}
    \mathbf{\mu}_k = \frac{1}{N_k} \sum_{i,l_i=k}{\mathbf{x}_i}, k=1,...,K, 
\end{equation}

Having calculated the centroids, a new observation $\mathbf{x}_*$ can be calculated based on the minimal distance from $\mathbf{\mu}_k$: 

\begin{equation}
    d(\mathbf{x}_*, \mathbf{\mu}_k) = ||\mathbf{x}_*-\mathbf{\mu}_k||^2_2
\end{equation}

\subsection{Nearest sub-class Centroid}
Nearest sub-class centroid is similar to the Nearest Centroid. It introduces a new sub-class element, that divides each class in a set amount of subclasses. Then during training the centroids of the subclasses are calculated. During predictions an observation is classified to the class which subclass it had the minimal distance. Given a set of N samples, where each sample is represented by a vector $\mathbf{x}_i\in\mathbbm{R}^D$ and the corresponding labels are denoted as $l_i$. Having a sub classes $m$ of class $c_k$ the centroids of each class can then be calculated\cite{ai-course-notes}: 

\begin{equation}
    \mathbf{\mu}_{km} = \frac{1}{N_{km}} \sum_{i,l_i=k, q_i=m}{\mathbf{x}_i}, k=1,...,K,
\end{equation}

The subclass label of vector $\mathbf{x}_i$ is $q_i$ and $N_{km}$ is the amount of samples of class $c_k$ composing subclass m. Classification on observations can then be done based on distance similar to the Nearest Centroid: 

\begin{equation}
    d(\mathbf{x}_*, \mathbf{\mu}_{km}) = ||\mathbf{x}_*-\mathbf{\mu}_{km}||^2_2
\end{equation}

\subsection{Nearest Neighbor}
The Nearest Neighbor schema stores the training data and classifies observations based on which training data samples they are closest to. The amount of neighbors to specify is highly dependent on the data set. A high amount of neighbors reduces the effect of bad samples and noise\cite{nearest-neighbor}. Meanwhile a low amount of neighbors leads to spurious classifications of possible outliers. 

\subsection{Perceptron using Backpropagation or MSE}
Perceptron is a (binary) linear classifier that decides if an observation is in a class or not. As an example looking at only two classes $c_1$ and $c_2$. Given a weight vector $\mathbf{w}$ and a input vector $\mathbf{x}\in\mathbbm{R}^D$ then a linear discrimination function g is: 

\begin{equation}
    g(\mathbf{x})=\mathbf{x}^Tx + w_0 = \sum_{d=1}^D{w_dx_d+w_0}
\end{equation}

$x_0$ is called the bias. The class decision of $\mathbf{x}$ is then dependent on the result of $g(\mathbf{x})$: 

\begin{equation}
    l_i=
    \begin{cases}
        c_1& \text{if $g(x)>0$} \\
        c_2& \text{if $g(x)<0$}  \\
        c_1 \lor c_2& \text{if $g(x)=0$}
    \end{cases}
\end{equation}

The weights are updated after each sample: 

\begin{equation}
    \mathbf{w}(t+1) = \mathbf{w}(t) - \eta(t) {\Delta} J_p = \mathbf{x}(t) + \eta(t) \sum_{\mathbf{x}_i \in X}{l_i \mathbf{x}_i}
\end{equation}

Where $\eta(t)$ is the learning rate that is either constant during all samples or are being according to a certain algorithm. If the learning rate is to high the classifier will converge to fast and the decision boundaries will be incorrect. Otherwise if it's to low the classifier will be slow. $\Delta J_p$ is the gradient of our optimization problem. As for utilizing this classifier for a problem with more than two classes we need to combine multiple binary classifiers. One option is the one-versus-all. If there is K classes then we need K binary classifiers. First one that determines if one class is different from the others, and a second one that determines if the second class is different from the others, and so on. This utilizes backpropagation by updating $\mathbf{w}$ such that it will be more fit to handle a similar input the next time\cite{ai-course-notes}.     

Instead of utilizing backpropagation the Minimum Squared Error can be used. The goal is still to update $\mathbf{w}$ as optimal as possible. This is done by minimizing an error vector $\mathbf{e}$:

\begin{equation}
    \mathbf{e} = \mathbf{X}^T \mathbf{w} - \mathbf{b}
\end{equation}

Where X is a matrix with columns containing the training vectors and $\mathbf{b}$ is vector with the target values. Solving the optimization problem leads to\cite{ai-course-notes}: 


\begin{equation}
    \mathbf{w} = (\mathbf{X} \mathbf{X})^{-1} \mathbf{X} \mathbf{b} = \mathbf{X}^\dagger \mathbf{b}
\end{equation}

The backpropagation solution fits well when clear decision boundaries can be drawn between the classes. If that is not the case utilizing Minimum Square Error can be more beneficial. 

\subsection{Principal component analysis}
There is advantages and disadvantages in reducing the dimensions of the data. It makes it easier to visualize the data. Having the feature reduction as a preprocessing step the time of training and testing classifiers will reduce. As all data can not be covered in the reduced dimensions it will also result in reduced accuracy when performing classification. The goal in reducing features is keeping it uncorrelated, meaning it should be possible to still differ which class each sample belongs to. PCA works by the first dimension having the largest possible variance. Then remembering the first dimension each succeeding dimension is also to have the largest possible variance\cite{ai-course-notes}.  